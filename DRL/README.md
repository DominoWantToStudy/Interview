### 1.梯度消失
由于sigmoid函数求偏导后公式形式的问题，当函数值接近0或1时梯度会非常小，网络很深的情况下模型靠近输入部分的参数很难被更新，模型就训练不起来；可以使用ReLU函数解决这个问题，因此现在很多深度模型往往在隐藏层中使用ReLU而不是Sigmoid（看具体公式推导）。
### 2.随机梯度下降
Motivation: 梯度下降算法中如果数据集较大，则每次迭代中计算损失函数的计算开销都比较大，因此需要随机梯度下降法来解决问题；  
主要思路：计算损失函数时不选用所有训练数据而只随机选取一小部分训练样本，这些小样本称为小批量(Mini-Batch)在，如果迭代次数足够多，小批量就能覆盖完整数据集。
### 3.自适应学习率
主要思路：如果当前处于一个较小的梯度上时，算法选择更大的步长；反之如果当前梯度较大，就给出一个较小的步长；  
Adam是最常见的自适应学习率算法，其计算梯度的一阶动量和二阶动量，进而分别获得一阶动量和二阶动量的滑动平均值，并且有两个因子β1和β2称为梯度的遗忘因子，就是同时考虑上一轮迭代结果的动量与这一轮迭代的梯度，两个β用于调整二者的权重，一般会设置接近1来重点考虑上一轮迭代结果的动量而不直接用当前的梯度来更新参数（这里动量可以理解为已进行的迭代过程获得的梯度的加权平均，越临近的历史梯度权重越大）。
### 4.正则化
把那些使得模型在训练集和测试集上都有很好效果的方法称为正则化方法，包括权重衰减、Dropout和标准批化。
#### 4.1 权重衰减
通过使多项式模型中各项的参数绝对值缩小，则曲线上下摆动的幅度会更小，以此减小过拟合，具体实现方法是给损失函数添加一个参数范式惩戒函数Ω，它的参数就是多项式模型各项的参数θ，并引入参数λ来控制参数范式惩戒的幅度，λ一般是一个较小的值。

![](https://latex.codecogs.com/svg.image?L_{total}=L(y,\widehat{y})&plus;\lambda&space;\Omega&space;(\Theta&space;))

![](https://github.com/DominoWantToStudy/Interview/blob/master/pic/%E5%8F%82%E6%95%B0%E8%8C%83%E5%BC%8F%E6%83%A9%E6%88%92%E5%87%BD%E6%95%B01.PNG)

![](https://github.com/DominoWantToStudy/Interview/blob/master/pic/%E5%8F%82%E6%95%B0%E8%8C%83%E5%BC%8F%E6%83%A9%E6%88%92%E5%87%BD%E6%95%B02.PNG)
#### 4.2 Dropout

#### 4.3 标准批化
