### 1.梯度消失
由于sigmoid函数求偏导后公式形式的问题，当函数值接近0或1时梯度会非常小，网络很深的情况下模型靠近输入部分的参数很难被更新，模型就训练不起来；可以使用ReLU函数解决这个问题，因此现在很多深度模型往往在隐藏层中使用ReLU而不是Sigmoid（看具体公式推导）。
### 2.随机梯度下降
Motivation: 梯度下降算法中如果数据集较大，则每次迭代中计算损失函数的计算开销都比较大，因此需要随机梯度下降法来解决问题；  
主要思路：计算损失函数时不选用所有训练数据而只随机选取一小部分训练样本，这些小样本称为小批量(Mini-Batch)在，如果迭代次数足够多，小批量就能覆盖完整数据集。
### 3.自适应学习率
主要思路：如果当前处于一个较小的梯度上时，算法选择更大的步长；反之如果当前梯度较大，就给出一个较小的步长；  
Adam是最常见的自适应学习率算法，其计算梯度的一阶动量和二阶动量，进而分别获得一阶动量和二阶动量的滑动平均值，并且有两个因子β1和β2称为梯度的遗忘因子，就是同时考虑上一轮迭代结果的动量与这一轮迭代的梯度，两个β用于调整二者的权重，一般会设置接近1来重点考虑上一轮迭代结果的动量而不直接用当前的梯度来更新参数（这里动量可以理解为已进行的迭代过程获得的梯度的加权平均，越临近的历史梯度权重越大）。
### 4.正则化
把那些使得模型在训练集和测试集上都有很好效果的方法称为正则化方法，包括权重衰减、Dropout和标准批化。
#### 4.1 权重衰减
通过使多项式模型中各项的参数绝对值缩小，则曲线上下摆动的幅度会更小，以此减小过拟合，具体实现方法是给损失函数添加一个参数范式惩戒函数Ω，它的参数就是多项式模型各项的参数θ，并引入参数λ来控制参数范式惩戒的幅度，λ一般是一个较小的值。

![](https://latex.codecogs.com/svg.image?L_{total}=L(y,\widehat{y})&plus;\lambda&space;\Omega&space;(\Theta&space;))

![](https://github.com/DominoWantToStudy/Interview/blob/master/pic/%E5%8F%82%E6%95%B0%E8%8C%83%E5%BC%8F%E6%83%A9%E6%88%92%E5%87%BD%E6%95%B01.PNG)

![](https://github.com/DominoWantToStudy/Interview/blob/master/pic/%E5%8F%82%E6%95%B0%E8%8C%83%E5%BC%8F%E6%83%A9%E6%88%92%E5%87%BD%E6%95%B02.PNG)
#### 4.2 Dropout
  当神经元数量过多时，神经网络会出现共适应问题，从而产生过拟合。神经元的共适应指的是神经元之间会互相依赖，一旦有一个神经元失效了，所有依赖它的神经元可能都会失效导致整个网络瘫痪。  
  Dropout方法是在训练过程中，将隐藏层的输出按比例随机设置为0，每一层有几个神经元随机地失去与其它层的连接，当然反向传播过程中如果输出为0则对应层的偏导数也会为0，只有还有连接的神经元会被更新。因此Dropout其实是训练很多个参数共享的小网络。  
  测试过程中不采用Dropout，所有神经元的输出都不会被设置为0，即所有训练好的小网络一起来做预测，与 __集成学习(Ensemble Learning)__ 类似，EL是训练多个模型来做同一个任务，在测试的时候用所有模型的输出来提高准确率。

![](https://github.com/DominoWantToStudy/Interview/blob/master/pic/Dropout.PNG)

#### 4.3 标准批化
