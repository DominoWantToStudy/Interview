### 1.梯度消失
由于sigmoid函数求偏导后公式形式的问题，当函数值接近0或1时梯度会非常小，网络很深的情况下模型靠近输入部分的参数很难被更新，模型就训练不起来；可以使用ReLU函数解决这个问题，因此现在很多深度模型往往在隐藏层中使用ReLU而不是Sigmoid（看具体公式推导）
### 2.随机梯度下降
Motivation: 梯度下降算法中如果数据集较大，则每次迭代中计算损失函数的计算开销都比较大，因此需要随机梯度下降法来解决问题  
算法核心思想就是计算损失函数时不选用所有训练数据而只随机选取一小部分训练样本，这些小样本称为小批量(Mini-Batch)
